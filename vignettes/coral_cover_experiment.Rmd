---
title: "Coral Cover Experiment"
author: "Jon Peppinck"
date: "25/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 2D Coral Cover

## 2D Training

1. Set the working directory to ./r2vr

2. Open and source `new_2D_coral_cover.R`

3. Setup scene:

* Enter the full name of the user separated by a hyphen

```{r}
user <- "Jon-Peppinck"
```
* Save the 2D images into `./2dimages/latest`

* By default the software assumes a consistent 4:3 aspect ratio for images, Ensure images are of these dimensions to avoid scaling issues e.g. 4000x3000 (4000px wide, 3000px tall)

* Provide up to 20 coordinates for each image in a list of lists data structure

```{r}
img1Points = list(
  list(x = 3203, y = 173),
  list(x = 1726, y = 356),
  list(x = 2291, y = 1086)
)

img2Points = list(
  list(x = 1000, y = 1000),
  list(x = 2000, y = 2000)
)
```

* Provide the corresponding result for each marker where isCoral is 0 or 1 to indicate not coral and is coral respectively for the corresponding ordered coordinates 

```{r}
img1PointsIsCoral = list(
  list(id = 1, isCoral = 0),
  list(id = 2, isCoral = 0), 
  list(id = 3, isCoral = 0) 
)

img2PointsIsCoral = list(
  list(id = 1, isCoral = 0),
  list(id = 2, isCoral = 0)
)

```

* Create a data structure for the image paths with its corresponding fixed coordinates and expected result

```{r}
img_paths <- list(
  list(img = "./2dimages/latest/49001074001.jpeg", imgPoints = img1Points, imgPointIsCoral =          img1PointsIsCoral),
  list(img = "./2dimages/latest/49002256001.jpeg", imgPoints = img2Points, imgPointIsCoral =          img2PointsIsCoral)
)
```

4. Start the VR server.

start()

In this experiment, there are 6 2D images in total. To avoid order bias, 3 out of the 6 images have been selected at random.

img_paths <- sample(img_paths, 3, replace=FALSE)

The imageXPoints variable is assigned to always correspond to the random image selected - Where X = 1, 2, 3. E.g. image1Points refers to the image points set at the beggining of Step 3 for whatever the first random image happens to be.

5. Call the fixed points for the first image

fixedPointsTemp(image1Points)

This will render the markers for the user to annotate.

6. The user can use the VR headset to align the camera focus point with a marker. They can then click a button on the headset to get a pop-up menu where they can select either 'C' for 
Coral or 'N' for not Coral by hovering over that option

7. Call the next image to annotate then repeat Step 5 & 6 until all 3 images are annotated

goImage(2)

fixedPointsTemp(image2Points)

goImage(3)

fixedPointsTemp(image1Points)

8. Give feedback to the user by calling check for each image in order

check(1)

check(2)

check(3)

9. Get feedback from the user by asking them evaluation question/(s)

question(1)

question(2)
